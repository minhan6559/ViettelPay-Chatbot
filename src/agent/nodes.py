"""
LangGraph Agent State and Processing Nodes
"""

from typing import Dict, List, Optional, TypedDict, Annotated
from langchain.schema import Document
from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
import json
import re


class ViettelPayState(TypedDict):
    """State for ViettelPay agent workflow with message history support"""

    # Message history for multi-turn conversation
    messages: Annotated[List[AnyMessage], add_messages]

    # Processing
    intent: Optional[str]
    confidence: Optional[float]

    # Query enhancement
    enhanced_query: Optional[str]

    # Knowledge retrieval
    retrieved_docs: Optional[List[Document]]

    # Response type metadata
    response_type: Optional[str]  # "script" or "generated"

    # Metadata
    error: Optional[str]
    processing_info: Optional[Dict]


def get_conversation_context(messages: List[AnyMessage], max_messages: int = 6) -> str:
    """
    Extract conversation context from message history

    Args:
        messages: List of conversation messages
        max_messages: Maximum number of recent messages to include

    Returns:
        Formatted conversation context string
    """
    if len(messages) <= 1:
        return ""

    context = "\n\nLá»‹ch sá»­ cuá»™c há»™i thoáº¡i:\n"
    # Get recent messages (excluding the current/last message for intent classification)
    recent_messages = messages[
        -(max_messages + 1) : -1
    ]  # Exclude the very last message

    for msg in recent_messages:
        # Handle different message types more robustly
        if hasattr(msg, "type"):
            if msg.type == "human":
                role = "NgÆ°á»i dÃ¹ng"
            elif msg.type == "ai":
                role = "Trá»£ lÃ½"
            else:
                role = f"Unknown-{msg.type}"
        elif hasattr(msg, "role"):
            if msg.role in ["user", "human"]:
                role = "NgÆ°á»i dÃ¹ng"
            elif msg.role in ["assistant", "ai"]:
                role = "Trá»£ lÃ½"
            else:
                role = f"Unknown-{msg.role}"
        else:
            role = "Unknown"

        # Limit message length to avoid token overflow
        # content = msg.content[:1000] + "..." if len(msg.content) > 1000 else msg.content
        content = msg.content
        context += f"{role}: {content}\n"
    # print(context)
    return context


def classify_intent_node(state: ViettelPayState, llm_client) -> ViettelPayState:
    """Node for intent classification using LLM with conversation context"""

    # Get the latest user message
    messages = state["messages"]
    if not messages:
        return {
            **state,
            "intent": "unclear",
            "confidence": 0.0,
            "error": "No messages found",
        }

    # Find the last human/user message
    user_message = None
    for msg in reversed(messages):
        if hasattr(msg, "type") and msg.type == "human":
            user_message = msg.content
            break
        elif hasattr(msg, "role") and msg.role == "user":
            user_message = msg.content
            break

    if not user_message:
        return {
            **state,
            "intent": "unclear",
            "confidence": 0.0,
            "error": "No user message found",
        }

    try:
        # Get conversation context for better intent classification
        conversation_context = get_conversation_context(messages)

        # Intent classification prompt with context
        classification_prompt = f"""
Báº¡n lÃ  há»‡ thá»‘ng phÃ¢n loáº¡i Ã½ Ä‘á»‹nh cho ViettelPay Pro. PhÃ¢n tÃ­ch tin nháº¯n cá»§a ngÆ°á»i dÃ¹ng vÃ  tráº£ vá» Ã½ Ä‘á»‹nh chÃ­nh.

CÃ¡c loáº¡i Ã½ Ä‘á»‹nh:
- greeting: ChÃ o há»i, báº¯t Ä‘áº§u cuá»™c trÃ² chuyá»‡n
- faq: CÃ¢u há»i vá» dá»‹ch vá»¥, tÃ­nh nÄƒng, thÃ´ng tin ViettelPay
- error_help: BÃ¡o lá»—i, cáº§n há»— trá»£ xá»­ lÃ½ lá»—i, mÃ£ lá»—i
- procedure_guide: Há»i cÃ¡ch thá»±c hiá»‡n má»™t thao tÃ¡c, quy trÃ¬nh
- policy_info: Há»i vá» quy Ä‘á»‹nh, chÃ­nh sÃ¡ch
- human_request: YÃªu cáº§u nÃ³i chuyá»‡n vá»›i ngÆ°á»i tháº­t, tÆ° váº¥n viÃªn
- out_of_scope: CÃ¢u há»i ngoÃ i pháº¡m vi ViettelPay (thá»i tiáº¿t, chÃ­nh trá»‹, v.v.)
- unclear: CÃ¢u há»i khÃ´ng rÃµ rÃ ng, khÃ´ng hiá»ƒu

VÃ­ dá»¥:
- "mÃ£ lá»—i 606 lÃ  gÃ¬" â†’ error_help
- "lÃ m sao náº¡p cÆ°á»›c" â†’ procedure_guide  
- "ViettelPay há»— trá»£ máº¡ng nÃ o" â†’ faq
- "lÃ m sao kháº¯c phá»¥c?" (sau khi há»i vá» lá»—i) â†’ error_help
- "tÃ´i váº«n chÆ°a hiá»ƒu" (sau cÃ¢u tráº£ lá»i) â†’ yÃªu cáº§u giáº£i thÃ­ch thÃªm, phá»¥ thuá»™c vÃ o ngá»¯ cáº£nh

{conversation_context}

Tin nháº¯n hiá»‡n táº¡i: "{user_message}"

HÃ£y phÃ¢n tÃ­ch dá»±a trÃªn cáº£ ngá»¯ cáº£nh cuá»™c há»™i thoáº¡i vÃ  tin nháº¯n hiá»‡n táº¡i.

QUAN TRá»ŒNG: Chá»‰ tráº£ vá» JSON thuáº§n tÃºy, khÃ´ng cÃ³ text khÃ¡c. Format chÃ­nh xÃ¡c:
{{"intent": "tÃªn_Ã½_Ä‘á»‹nh", "confidence": 0.9, "explanation": "lÃ½ do ngáº¯n gá»n"}}
"""

        # Get classification using the pre-initialized LLM client
        response = llm_client.generate(classification_prompt, temperature=0.1)

        print(f"ðŸ” Raw LLM response: {response}")

        # Parse JSON response
        try:
            # Try to extract JSON from response (in case there's extra text)
            response_clean = response.strip()

            # Look for JSON object in the response
            json_match = re.search(r"\{.*\}", response_clean, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
            else:
                # Try parsing the whole response
                result = json.loads(response_clean)

            intent = result.get("intent", "unclear")
            confidence = result.get("confidence", 0.5)
            explanation = result.get("explanation", "")

            print(
                f"âœ… JSON parsed successfully: intent={intent}, confidence={confidence}"
            )

        except (json.JSONDecodeError, AttributeError) as e:
            print(f"âŒ JSON parsing failed: {e}")
            print(f"   Raw response: {response}")

            # Fallback: try to extract intent from text
            response_lower = response.lower()
            if any(
                word in response_lower for word in ["lá»—i", "error", "606", "mÃ£ lá»—i"]
            ):
                intent = "error_help"
                confidence = 0.7
            elif any(word in response_lower for word in ["xin chÃ o", "hello", "chÃ o"]):
                intent = "greeting"
                confidence = 0.8
            elif any(word in response_lower for word in ["há»§y", "cancel", "thá»§ tá»¥c"]):
                intent = "procedure_guide"
                confidence = 0.7
            elif any(
                word in response_lower for word in ["náº¡p", "cÆ°á»›c", "dá»‹ch vá»¥", "faq"]
            ):
                intent = "faq"
                confidence = 0.7
            else:
                intent = "unclear"
                confidence = 0.3

            print(f"ðŸ”„ Fallback classification: {intent} (confidence: {confidence})")
            explanation = "Fallback classification due to JSON parse error"

        print(f"ðŸŽ¯ Intent classified: {intent} (confidence: {confidence})")

        return {
            **state,
            "intent": intent,
            "confidence": confidence,
            "processing_info": {
                "classification_raw": response,
                "explanation": explanation,
                "context_used": bool(conversation_context.strip()),
            },
        }

    except Exception as e:
        print(f"âŒ Intent classification error: {e}")
        return {**state, "intent": "unclear", "confidence": 0.0, "error": str(e)}


def query_enhancement_node(state: ViettelPayState, llm_client) -> ViettelPayState:
    """Node for enhancing search query using conversation context"""

    # Get the latest user message
    messages = state["messages"]
    if not messages:
        return {**state, "enhanced_query": "", "error": "No messages found"}

    # Find the last human/user message
    user_message = None
    for msg in reversed(messages):
        if hasattr(msg, "type") and msg.type == "human":
            user_message = msg.content
            break
        elif hasattr(msg, "role") and msg.role == "user":
            user_message = msg.content
            break

    if not user_message:
        return {**state, "enhanced_query": "", "error": "No user message found"}

    try:
        # Get conversation context
        conversation_context = get_conversation_context(messages)

        # If no context, use original message
        if not conversation_context.strip():
            print(f"ðŸ” No context available, using original query: {user_message}")
            return {**state, "enhanced_query": user_message}

        # Query enhancement prompt
        enhancement_prompt = f"""
Báº¡n lÃ  há»‡ thá»‘ng táº¡o truy váº¥n tÃ¬m kiáº¿m thÃ´ng minh cho ViettelPay Pro. HÃ£y táº¡o truy váº¥n tÃ¬m kiáº¿m tá»‘i Æ°u dá»±a trÃªn toÃ n bá»™ ngá»¯ cáº£nh cuá»™c trÃ² chuyá»‡n.

{conversation_context}

Tin nháº¯n hiá»‡n táº¡i: "{user_message}"

Nhiá»‡m vá»¥: Táº¡o truy váº¥n tÃ¬m kiáº¿m chi tiáº¿t vÃ  thÃ´ng minh báº±ng cÃ¡ch:

1. **PhÃ¢n tÃ­ch ngá»¯ cáº£nh tá»•ng thá»ƒ:**
   - Káº¿t há»£p thÃ´ng tin tá»« bá»‘i cáº£nh cuá»™c trÃ² chuyá»‡n trÆ°á»›c
   - Hiá»ƒu má»‘i liÃªn há»‡ giá»¯a tin nháº¯n hiá»‡n táº¡i vÃ  chá»§ Ä‘á» Ä‘Ã£ tháº£o luáº­n
   - Nháº­n diá»‡n chuá»—i váº¥n Ä‘á» hoáº·c yÃªu cáº§u liÃªn quan

2. **Má»Ÿ rá»™ng vÃ  lÃ m rÃµ truy váº¥n:**
   - Thay tháº¿ Ä‘áº¡i tá»« ("nÃ³", "tháº¿", "váº­y", "Ä‘Ã³") hoáº·c cÃ¡c tá»« khÃ´ng rÃµ rÃ ng báº±ng Ä‘á»‘i tÆ°á»£ng cá»¥ thá»ƒ tá»« ngá»¯ cáº£nh
   - ThÃªm tá»« khÃ³a liÃªn quan vÃ  thuáº­t ngá»¯ chuyÃªn mÃ´n ViettelPay
   - Bao gá»“m cÃ¡c biáº¿n thá»ƒ cÃ¡ch diá»…n Ä‘áº¡t vÃ  tá»« Ä‘á»“ng nghÄ©a
   - Cá»¥ thá»ƒ hÃ³a cÃ¡c yÃªu cáº§u mÆ¡ há»“ dá»±a trÃªn ngá»¯ cáº£nh

3. **Tá»‘i Æ°u cho tÃ¬m kiáº¿m:**
   - Sá»­ dá»¥ng thuáº­t ngá»¯ chÃ­nh xÃ¡c cá»§a ViettelPay (giao dá»‹ch, náº¡p cÆ°á»›c, OTP, mÃ£ lá»—i...)
   - Sá»­ dá»¥ng tá»« Ä‘á»“ng nghÄ©a vÃ  cÃ¡ch diá»…n Ä‘áº¡t khÃ¡c nhau trong tiáº¿ng Viá»‡t Ä‘á»ƒ tÄƒng kháº£ nÄƒng tÃ¬m kiáº¿m
   - ThÃªm cÃ¡c tá»« khÃ³a vÃ  cá»¥m tá»« cÃ³ kháº£ nÄƒng xuáº¥t hiá»‡n trong tÃ i liá»‡u hÆ°á»›ng dáº«n vÃ o má»™t cÃ¢u riÃªng cuá»‘i truy váº¥n
   - Duy trÃ¬ tÃ­nh tá»± nhiÃªn cá»§a tiáº¿ng Viá»‡t
   - Æ¯u tiÃªn Ä‘á»™ chÃ­nh xÃ¡c vÃ  liÃªn quan

QUAN TRá»ŒNG: 
- Truy váº¥n pháº£i pháº£n Ã¡nh Ä‘Ãºng Ã½ Ä‘á»‹nh vÃ  ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§
- KhÃ´ng thÃªm thÃ´ng tin khÃ´ng cÃ³ trong ngá»¯ cáº£nh
- Duy trÃ¬ tÃ­nh tá»± nhiÃªn cá»§a tiáº¿ng Viá»‡t
- Táº­p trung vÃ o viá»‡c tÃ¬m kiáº¿m thÃ´ng tin há»— trá»£ cá»¥ thá»ƒ

CHá»ˆ tráº£ vá» truy váº¥n Ä‘Æ°á»£c tÄƒng cÆ°á»ng, khÃ´ng cÃ³ giáº£i thÃ­ch.
"""

        # Get enhanced query
        enhanced_query = llm_client.generate(enhancement_prompt, temperature=0.1)
        enhanced_query = enhanced_query.strip()

        print(f"ðŸ” Original query: {user_message}")
        print(f"ðŸš€ Enhanced query: {enhanced_query}")

        return {**state, "enhanced_query": enhanced_query}

    except Exception as e:
        print(f"âŒ Query enhancement error: {e}")
        # Fallback to original message
        return {**state, "enhanced_query": user_message, "error": str(e)}


def knowledge_retrieval_node(
    state: ViettelPayState, knowledge_retriever
) -> ViettelPayState:
    """Node for knowledge retrieval using pre-initialized ViettelKnowledgeBase"""

    # Use enhanced query if available, otherwise fall back to extracting from messages
    enhanced_query = state.get("enhanced_query", "")

    if not enhanced_query:
        # Fallback: extract from messages
        messages = state["messages"]
        if not messages:
            return {**state, "retrieved_docs": [], "error": "No messages found"}

        # Find the last human/user message
        for msg in reversed(messages):
            if hasattr(msg, "type") and msg.type == "human":
                enhanced_query = msg.content
                break
            elif hasattr(msg, "role") and msg.role == "user":
                enhanced_query = msg.content
                break

    if not enhanced_query:
        return {**state, "retrieved_docs": [], "error": "No query available"}

    try:
        if not knowledge_retriever:
            raise ValueError("Knowledge retriever not available")

        # Retrieve relevant documents using enhanced query and pre-initialized ViettelKnowledgeBase
        retrieved_docs = knowledge_retriever.search(enhanced_query, top_k=10)

        print(
            f"ðŸ“š Retrieved {len(retrieved_docs)} documents for enhanced query: {enhanced_query}"
        )

        return {**state, "retrieved_docs": retrieved_docs}

    except Exception as e:
        print(f"âŒ Knowledge retrieval error: {e}")
        return {**state, "retrieved_docs": [], "error": str(e)}


def script_response_node(state: ViettelPayState) -> ViettelPayState:
    """Node for script-based responses"""

    from src.agent.scripts import ConversationScripts
    from langchain_core.messages import AIMessage

    intent = state.get("intent", "")

    try:
        # Load scripts
        scripts = ConversationScripts("./viettelpay_docs/processed/kich_ban.csv")

        # Map intents to script types
        intent_to_script = {
            "greeting": "greeting",
            "out_of_scope": "out_of_scope",
            "human_request": "human_request_attempt_1",  # Could be enhanced later
            "unclear": "ask_for_clarity",
        }

        script_type = intent_to_script.get(intent)

        if script_type and scripts.has_script(script_type):
            response_text = scripts.get_script(script_type)
            print(f"ðŸ“‹ Using script response: {script_type}")

            # Add AI message to the conversation
            ai_message = AIMessage(content=response_text)

            return {**state, "messages": [ai_message], "response_type": "script"}

        else:
            # Fallback script
            fallback_response = (
                "Xin lá»—i, em chÆ°a hiá»ƒu rÃµ yÃªu cáº§u cá»§a anh/chá»‹. Vui lÃ²ng thá»­ láº¡i."
            )
            ai_message = AIMessage(content=fallback_response)

            print(f"ðŸ“‹ Using fallback script for intent: {intent}")

            return {**state, "messages": [ai_message], "response_type": "script"}

    except Exception as e:
        print(f"âŒ Script response error: {e}")
        fallback_response = "Xin lá»—i, em gáº·p lá»—i ká»¹ thuáº­t. Vui lÃ²ng thá»­ láº¡i sau."
        ai_message = AIMessage(content=fallback_response)

        return {
            **state,
            "messages": [ai_message],
            "response_type": "error",
            "error": str(e),
        }


def generate_response_node(state: ViettelPayState, llm_client) -> ViettelPayState:
    """Node for LLM-based response generation with conversation context"""

    from langchain_core.messages import AIMessage

    # Get the latest user message and conversation history
    messages = state["messages"]
    if not messages:
        ai_message = AIMessage(content="Xin lá»—i, em khÃ´ng thá»ƒ xá»­ lÃ½ yÃªu cáº§u nÃ y.")
        return {**state, "messages": [ai_message], "response_type": "error"}

    # Find the last human/user message
    user_message = None
    for msg in reversed(messages):
        if hasattr(msg, "type") and msg.type == "human":
            user_message = msg.content
            break
        elif hasattr(msg, "role") and msg.role == "user":
            user_message = msg.content
            break

    if not user_message:
        ai_message = AIMessage(content="Xin lá»—i, em khÃ´ng thá»ƒ xá»­ lÃ½ yÃªu cáº§u nÃ y.")
        return {**state, "messages": [ai_message], "response_type": "error"}

    intent = state.get("intent", "")
    retrieved_docs = state.get("retrieved_docs", [])
    enhanced_query = state.get("enhanced_query", "")

    try:
        # Build context from retrieved documents
        context = ""
        if retrieved_docs:
            context = "\n\n".join(
                [
                    f"[{doc.metadata.get('doc_type', 'unknown')}] {doc.page_content}"
                    for doc in retrieved_docs
                ]
            )

        # Get conversation context using the helper function
        conversation_context = get_conversation_context(messages, max_messages=6)

        # Generate response prompt based on intent
        if intent == "error_help":
            system_prompt = """Báº¡n lÃ  chuyÃªn gia há»— trá»£ ká»¹ thuáº­t ViettelPay Pro.
Thá»ƒ hiá»‡n sá»± cáº£m thÃ´ng vá»›i khÃ³ khÄƒn cá»§a ngÆ°á»i dÃ¹ng.
Cung cáº¥p giáº£i phÃ¡p cá»¥ thá»ƒ, tá»«ng bÆ°á»›c.
Náº¿u cáº§n há»— trá»£ thÃªm, hÆ°á»›ng dáº«n liÃªn há»‡ tá»•ng Ä‘Ã i.
Náº¿u cÃ³ lá»‹ch sá»­ cuá»™c há»™i thoáº¡i, hÃ£y tham kháº£o Ä‘á»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i phÃ¹ há»£p vÃ  cÃ³ tÃ­nh liÃªn káº¿t."""

        elif intent == "procedure_guide":
            system_prompt = """Báº¡n lÃ  hÆ°á»›ng dáº«n viÃªn ViettelPay Pro.
Cung cáº¥p hÆ°á»›ng dáº«n tá»«ng bÆ°á»›c rÃµ rÃ ng.
Bao gá»“m link video náº¿u cÃ³ trong thÃ´ng tin.
Sá»­ dá»¥ng format cÃ³ sá»‘ thá»© tá»± cho cÃ¡c bÆ°á»›c.
Náº¿u cÃ³ lá»‹ch sá»­ cuá»™c há»™i thoáº¡i, hÃ£y tham kháº£o Ä‘á»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i phÃ¹ há»£p vÃ  cÃ³ tÃ­nh liÃªn káº¿t."""

        else:  # faq, policy_info, etc.
            system_prompt = """Báº¡n lÃ  trá»£ lÃ½ áº£o ViettelPay Pro.
Tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p.
Giá»ng Ä‘iá»‡u thÃ¢n thiá»‡n, chuyÃªn nghiá»‡p.
Sá»­ dá»¥ng "Anh/chá»‹" khi xÆ°ng hÃ´.
Náº¿u cÃ³ lá»‹ch sá»­ cuá»™c há»™i thoáº¡i, hÃ£y tham kháº£o Ä‘á»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i phÃ¹ há»£p vÃ  cÃ³ tÃ­nh liÃªn káº¿t."""

        # Build full prompt with both knowledge context and conversation context
        generation_prompt = f"""{system_prompt}

ThÃ´ng tin tham kháº£o tá»« cÆ¡ sá»Ÿ tri thá»©c:
{context}

{conversation_context}

CÃ¢u há»i hiá»‡n táº¡i cá»§a ngÆ°á»i dÃ¹ng: {user_message}
Truy váº¥n tÃ¬m kiáº¿m Ä‘Ã£ Ä‘Æ°á»£c cáº£i thiá»‡n: {enhanced_query}

HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin tham kháº£o vÃ  lá»‹ch sá»­ cuá»™c há»™i thoáº¡i (náº¿u cÃ³). Náº¿u khÃ´ng cÃ³ thÃ´ng tin phÃ¹ há»£p, hÃ£y nÃ³i ráº±ng báº¡n cáº§n thÃªm thÃ´ng tin.
"""

        # Generate response using the pre-initialized LLM client
        response_text = llm_client.generate(generation_prompt, temperature=0.1)

        print(f"ðŸ¤– Generated response for intent: {intent}")

        # Add AI message to the conversation
        ai_message = AIMessage(content=response_text)

        return {**state, "messages": [ai_message], "response_type": "generated"}

    except Exception as e:
        print(f"âŒ Response generation error: {e}")
        error_response = "Xin lá»—i, em gáº·p lá»—i khi xá»­ lÃ½ yÃªu cáº§u. Vui lÃ²ng thá»­ láº¡i sau."
        ai_message = AIMessage(content=error_response)

        return {
            **state,
            "messages": [ai_message],
            "response_type": "error",
            "error": str(e),
        }


# Routing function for conditional edges
def route_after_intent_classification(state: ViettelPayState) -> str:
    """Route to appropriate node after intent classification"""

    intent = state.get("intent", "unclear")

    # Script-based intents (no knowledge retrieval needed)
    script_intents = {"greeting", "out_of_scope", "human_request", "unclear"}

    if intent in script_intents:
        return "script_response"
    else:
        # Knowledge-based intents need query enhancement first
        return "query_enhancement"


def route_after_query_enhancement(state: ViettelPayState) -> str:
    """Route after query enhancement (always to knowledge retrieval)"""
    return "knowledge_retrieval"


def route_after_knowledge_retrieval(state: ViettelPayState) -> str:
    """Route after knowledge retrieval (always to generation)"""
    return "generate_response"
